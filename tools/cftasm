#!/usr/bin/env python3
# coding: utf-8
# -*- python -*-
"""
The CFT Assembler, version 4
"""

import os
import io
import re
import sys
import copy
import types
import array
import pprint
import doctest
import argparse
import subprocess
import unicodedata

# Microcode 7 instruction set. This is the current one, with Memory Bank
# Registers and a Stack Pointer.
_BUILTINS = """;;; CFT Assembler Built-ins, Microcode Version 7.x.

;;; Native instruction mnemonics

.equ    IRET    &0000            ; 0000:0:0:000:-------  Interrupt Return
.equ    LRET    &0080            ; 0000:0:0:001:-------  Long Return
.equ    RET     &0100            ; 0000:0:0:010:-------  Return
.equ    TAS     &0180            ; 0000:0:0:011:-------  Transfer Accumulator to Stack Pointer
.equ    TSA     &0200            ; 0000:0:0:100:-------  Transfer Stack Pointer to Accumulator
.equ    TAD     &0280            ; 0000:0:0:101:-------  Transfer Accumulator to Data Register
.equ    TDA     &0300            ; 0000:0:0:110:-------  Transfer Data Register to Accumulator
.equ    SWAB    &0380            ; 0000:0:0:111:-------  Swap high and low bytes
.equ    TRAP    &0400            ; 0000:0:1:000:LLLLLLL  Software Interrupt
.equ    PHA     &0480            ; 0000:0:1:001:-------  Push Accumulator
.equ    PPA     &0500            ; 0000:0:1:010:-------  Pop Accumulator
.equ    PHF     &0580            ; 0000:0:1:011:-------  Push Flags
.equ    PPF     &0600            ; 0000:0:1:100:-------  Pop Flags
.equ    STI     &0680            ; 0000:0:1:101:-------  Set Interrupt Flag
.equ    CLI     &0700            ; 0000:0:1:110:-------  Clear Interrupt Flag
.equ    WAIT    &0780            ; 0000:0:1:111:LLLLLLL  Wait for Interrupt
.equ    SHL     &0800            ; 0000:1:0:000:000LLLL  Bitwise Shift Left
.equ    SHR     &0810            ; 0000:1:0:000:001LLLL  Bitwise Shift Right
.equ    ASR     &0830            ; 0000:1:0:000:011LLLL  Arithmetic Shift Right
.equ    ROL     &0840            ; 0000:1:0:000:100LLLL  Roll Left
.equ    ROR     &0850            ; 0000:1:0:000:101LLLL  Roll Right

.equ    SKP     &0880            ; SKP bitmap instruction
.equ    NOP     SKP     #0000000 ; 0000:1:0:001:0000000  No Operation
.equ    SKIP    SKP     #0010000 ; 0000:1:0:001:--10000  Skip
.equ    SNP     SKP     #0001100 ; 0000:1:0:001:--011--  Skip on Non-Positive Accumulator
.equ    SNA     SKP     #0001000 ; 0000:1:0:001:--01---  Skip on Negative Accumulator
.equ    SZA     SKP     #0000100 ; 0000:1:0:001:--0-1--  Skip on Zero Accumulator
.equ    SSL     SKP     #0000010 ; 0000:1:0:001:--0--1-  Skip on Link
.equ    SSV     SKP     #0000001 ; 0000:1:0:001:--0---1  Skip on Overflow
.equ    SNN     SKP     #0011000 ; 0000:1:0:001:--11---  Skip on Non-Negative Accumulator
.equ    SPA     SKP     #0011100 ; 0000:1:0:001:--111--  Skip on Positive Accumulator
.equ    SNZ     SKP     #0010100 ; 0000:1:0:001:--1-1--  Skip on Non-Zero Accumulator
.equ    SCL     SKP     #0010010 ; 0000:1:0:001:--1--1-  Skip on Link Clear
.equ    SCV     SKP     #0010001 ; 0000:1:0:001:--1---1  Skip on Overflow
.equ    LCT     &0900            ; 0000:1:0:010:-------  Load Context Register
.equ    SCT     &0980            ; 0000:1:0:011:-------  Set Context Register
.equ    LMB     &0a00            ; 0000:1:0:100:----LLL  Read Memory Bank
.equ    SMB     &0a80            ; 0000:1:0:101:----LLL  Set Memory Bank
.equ    NMB     &0b00            ; 0000:1:0:110:----LLL  Write MBR for other context
.equ    ECT     &0b80            ; 0000:1:0:111:LLLLLLL  Enter Context
.equ    JPA     &0c00            ; 0000:1:1:000:-------  Jump to Accumulator
.equ    JSA     &0c80            ; 0000:1:1:001:-------  Jump to Subroutine at Accumulator
.equ    PEEK    &0d00            ; 0000:1:1:010:-------  Peek top of Stack
.equ    HCF     &0d80            ; 0000:1:1:011:-------  Halt and Catch Fire

.equ    UOP     &0e00            ; UOP bitmap instruction
.equ    NOP9    UOP     #0000000 ; 0000:1:1:100:0000000  No Operation, 9 Cycles
.equ    NEG     UOP     #0001100 ; 0000:1:1:100:---11--  Negate Accumulator
.equ    SEL     UOP     #0010001 ; 0000:1:1:100:--1---1  Set Link
.equ    CLA     UOP     #0100000 ; 0000:1:1:100:-1-----  Clear Accumulator
.equ    CLL     UOP     #0010000 ; 0000:1:1:100:--1----  Clear Link
.equ    NOT     UOP     #0001000 ; 0000:1:1:100:---1---  Complement Accumulator
.equ    INC     UOP     #0000100 ; 0000:1:1:100:----1--  Increment Accumulator
.equ    DEC     UOP     #0000010 ; 0000:1:1:100:-----1-  Decrement Accumulator
.equ    CPL     UOP     #0000001 ; 0000:1:1:100:------1  Complement Link

.equ    IFL     &0e80            ; IFL bitmap instruction

; Bitmap sub-instructions NEG, SEL, CLA, CLL, NOT, INC, DEC and CPL
; are available with IFL. Use them as IFL NEG.

; .equ  NEG     IFL     #0001100 ; 0000:1:1:101:---11--  Negate Accumulator If Link Set
; .equ  SEL     IFL     #0010001 ; 0000:1:1:101:--1---1  Set Link
; .equ  CLA     IFL     #0100000 ; 0000:1:1:101:-1-----  Clear Accumulator If Link Set
; .equ  CLL     IFL     #0010000 ; 0000:1:1:101:--1----  Clear Link if Link Set
; .equ  NOT     IFL     #0001000 ; 0000:1:1:101:---1---  Complement Accumulator If Link Set
; .equ  INC     IFL     #0000100 ; 0000:1:1:101:----1--  Increment Accumulator If Link Set
; .equ  DEC     IFL     #0000010 ; 0000:1:1:101:-----1-  Decrement Accumulator If Link Set
; .equ  CPL     IFL     #0000001 ; 0000:1:1:101:------1  Complement Link If Link Set ***

.equ  IFV     &0f00            ; IFV bitmap instruction

; Bitmap sub-instructions NEG, SEL, CLA, CLL, NOT, INC, DEC and CPL
; are available with IFL. Use them as IFL NEG.

; .equ  NEG     IFV     #0001100 ; 0000:1:1:110:---11--  Negate Accumulator on Overflow
; .equ  SEL     IFV     #0010001 ; 0000:1:1:110:--1---1  Set Link on Overflow
; .equ  CLA     IFV     #0100000 ; 0000:1:1:110:-1-----  Clear Accumulator on Overflow
; .equ  CLL     IFV     #0010000 ; 0000:1:1:110:--1----  Clear Link on Overflow
; .equ  NOT     IFV     #0001000 ; 0000:1:1:110:---1---  Complement Accumulator on Overflow
; .equ  INC     IFV     #0000100 ; 0000:1:1:110:----1--  Increment Accumulator on Overflow
; .equ  DEC     IFV     #0000010 ; 0000:1:1:110:-----1-  Decrement Accumulator on Overflow
; .equ  CPL     IFV     #0000001 ; 0000:1:1:110:------1  Complement Link on Overflow

.equ    IND     &0fc0            ; 0000:1:1:111:1---LLL  Indirect
.equ    LIA     &1000            ; 0001:I:R:mmmmmmmmmm   Literal Address
.equ    LI      &1400            ; 0001:0:1:LLLLLLLLLL   Literal
.equ    LJSR    &1800            ; 0001:1:R:aaaaaaaaaa   Long Jump to Subroutine
.equ    LJMP    &2800            ; 0010:1:R:aaaaaaaaaa   Long Jump
.equ    JSR     &3000            ; 0011:I:R:mmmmmmmmmm   Jump to Subroutine
.equ    JMP     &4000            ; 0100:I:R:mmmmmmmmmm   Jump
.equ    IN      &5000            ; 0101:I:R:mmmmmmmmmm   Input from I/O Space
.equ    OUT     &6000            ; 0110:I:R:mmmmmmmmmm   Output to I/O Space
.equ    IOT     &7000            ; 0111:I:R:mmmmmmmmmm   I/O Transaction
.equ    LOAD    &8000            ; 1000:I:R:mmmmmmmmmm   Load Accumulator
.equ    STORE   &9000            ; 1001:I:R:mmmmmmmmmm   Store Accumulator
.equ    DSZ     &a000            ; 1010:I:R:mmmmmmmmmm   Decrement and Skip if Zero
.equ    ADD     &c000            ; 1100:I:R:mmmmmmmmmm   Add To Accumulator
.equ    AND     &d000            ; 1101:I:R:mmmmmmmmmm   Bitwise AND With Accumulator
.equ    OR      &e000            ; 1110:I:R:mmmmmmmmmm   Bitwise OR With Accumulator
.equ    XOR     &f000            ; 1111:I:R:mmmmmmmmmm   Bitwise XOR With Accumulator


;;; 1.1.1. Instruction flags.

.equ R     0x0400 ; Specify to reference a register (zero page).
.equ I     0x0800 ; Indirection

;;; Macros for the Memory Bank Unit (MBU, built into the processor).

.equ    MBP     0                ; MB0 is MBP, the Program bank
.equ    MBD     1                ; MB1 is MBD, the Data bank
.equ    MBS     2                ; MB2 is MBS, the Stack bank
.equ    MBZ     3                ; MB3 is MBZ, the Page Zero bank

;;; End of file.
"""


class UndefinedSymbolException(Exception):
    pass


class Symbol(object):
    """A member of the symbol table."""
    def __init__(self, filename, linenum, name, val=None, tokens=None):
        self.filename = filename
        self.linenum = linenum
        self.name = name
        self.val = val
        self.tokens = tokens


    def __str__(self):
        return '<%s %s = %04x>' % (self.__class__.__name__, self.name, self.val)


    def __repr__(self):
        if self.val is None:
            return '<Unresolved_{} {}={}>'.format(self.__class__.__name__, self.name, self.tokens)
        else:
            return '<{} {}={} ({})>'.format(self.__class__.__name__, self.name, self.val, self.tokens)


    @property
    def masked_val(self):
        """Return the lower 10 bits of a value, for use in instruction operands. In
        generic symbols, this just returns the value.
        """
        return self.val


    @property
    def unresolved(self):
        return self.val is None


    @property
    def resolved(self):
        return self.val is not None


    # @property
    # def page(self):
    #     """Provide the middle 6 bits of the value."""
    #     return self.val & 0xfc00


    # @property
    # def offset(self):
    #     """Provide the lower 10 bits of the value."""
    #     return self.val & 0x03ff


    # @property
    # def assembly_val(self):
    #     """Provide a field for instruction assembly. For non-labels
    #     (i.e. instructions, EQUs, etc), we return the whole word."""
    #     return self.val


    # def inaccessible(self, addr):
    #     """Returns True if the value represents an address
    #     inaccessible from the current address due to them being on
    #     different pages.

    #     For non-label symbols, always return False. They don't
    #     necessarily represent addresses."""
    #     return False


class Label(Symbol):
    @property
    def masked_val(self):
        """Return the lower 10 bits of a value, for use in instruction operands."""
        return self.val & 0x3ff


class Register(Symbol):
    @property
    def masked_val(self):
        """Return the lower 10 bits of a value, for use in instruction operands."""
        return self.val & 0x3ff




class CFTAssembler(object):

    # The Assembler built-ins.
    BUILTINS = _BUILTINS

    # This protects against endless includes
    MAX_PREPROCESSOR_DEPTH = 20

    # Lexer states
    TOK_SP = 0                  # Start of line, scanning for word
    TOK_WORD = 1                # Start of a word
    TOK_QUOTE = 2               # Tokenising a quoted string
    TOK_QUOTEESC = 3            # Escape '\' character in quoted string
    TOK_PAREN = 4               # Look for first argument in arg list
    TOK_PARENWORD = 5           # Parsing argument in arg list
    TOK_PARENSEP = 6            # Looking for comma or ')' in arg lits
    TOK_ATEXPR = 7              # Found an @-expression
    TOK_ATEXPRLONG = 8          # Found an @{} expression

    # Lexer types in token lists
    TKT_WORD = 0                # A bare word was found
    TKT_STR = 1                 # A double-quoted string
    TKT_LPAREN = 2              # Left parenthesis
    TKT_ARGSEP = 3              # Argument separator
    TKT_RPAREN = 4              # Right parenthesis
    TKT_PARENSEP = 5            # Comma in a parenthetic list

    # Lexer: escape character map
    ESCAPED_CHARS = {
        "a": '\a',
        "b": '\b',
        "t": '\t',
        "n": '\n',
        "v": '\v',
        "f": '\f',
        "r": '\r',
        "@": '\0',
        }

    # Binary bitfield character mapping
    BINDICT = {
        ":": "", ",": "", "'": "",
        "0": "0", "-": "0", "_": "0", ".": "0",
        "1": "1"
    }

    # Assembly models
    MODEL_SHORT = 16
    MODEL_LONG = 24

    # Poke types. These help us figure out what each memory address is used
    # for. Note, 0 isn't used.
    PKT_UNUSED = 0
    PKT_INSTR = ord('I')
    PKT_DATA = ord('D')
    PKT_WORD = ord('W')
    PKT_FILL = ord('F')
    PKT_STR = ord('S')
    PKT_STRP = ord('P')
    PKT_REG = ord('R')
    PKT_UNRESOLVED = 254        # An unresolved symbol would go here
    PKT_UNKNOWN = 255

    POKETYPES = {
        PKT_UNUSED:     "not used",
        PKT_INSTR:      "instruction",
        PKT_DATA:       ".data block",
        PKT_WORD:       "word data",
        PKT_STR:        "string",
        PKT_STRP:       "packed string",
        PKT_REG:        "global register",
        PKT_UNRESOLVED: "undefined symbolic value",
    }
    

    def __init__(self):
        # self.linenum = 1
        # self.out = sys.stderr.write

        # # The symbol table.
        self.symbols = dict()

        # The namespace stack.
        self.ns = list()

        # The macro table and macro context stack
        self.macros = dict()

        # This is used by logasm so as not to repeat lines unnecessarily.
        self.logasm_state = (None, None)

        # self.mcallno = 0

        # self.regexp_hex = re.compile('^(?:&|0x)([0-9A-Fa-f]+)$')
        # self.regexp_bin = re.compile('^#([0-9A-Fa-f]+)$')

        # # Filename and line numbers.
        # self.filename = '<none>'
        # self.linenum = 1

        # self.opts, self.args = self.parseOpts()

        # # Create the processed assembly output file.

        # # State
        # self.wrap = False
        # self.jumped = False
        # self.bank = None
        
        # # Execute
        # try:
        #     self.run()
        # except IOError as e:
        #     self.die(str(e))

        # Valid Assembly directives
        self.DIRECTIVES = {
            '.bank': self.deprecated_directive,
            '.data': self.parse_dot_data,
            '.endstring': self.parse_dot_endstring,
            '.equ': self.parse_dot_equ,
            '.fill': self.parse_dot_fill,
            '.longstring': self.parse_dot_longstring,
            '.reg': self.parse_dot_reg,
            '.regfill': self.parse_dot_fill, # Same handler as .fill
            '.str': self.parse_dot_data,     # same handler as .data
            '.strp': self.parse_dot_strp,
            '.word': self.parse_dot_word,
            '.page': self.parse_dot_page,
            '.strn': None,

            # These are implemented in the preprocessor
            '.include': None,
            '.macro': None,
            '.endmacro': None,
            '.end': None,            # Deprecated
            '.popns': None,
            '.pushns': None,
            '.scope': None,
            '.endscope': None,
            }

        self.one_off_warnings = dict()

        


    def parse_command_line(self):
        """Parse command-line options."""
        p = argparse.ArgumentParser(description="An Assembler for the 2019 revision of the CFT.")

        p.add_argument('-o', '--out-file', metavar='OUTPUT-FILE', default='a.bin',
                     help='Output object file to OUTPUT-FILE (default: a.bin).')

        p.add_argument('-m', '--model', choices=["short", "long"],
                       default="short", help="""Choose an assembly model. --model=short produces
                       relocatable images that can be a maximum of 64KW (65,536 Words).
                       --model=long allows the full 24-bit memory to be written to.
                       This affects object code sizes, sanity checks, warnings and how
                       addresses are printed. (default: %(default)s)""")

        p.add_argument('-a', '--asm-file',
                       help='Set processed assembly file output (default: OUTPUT-FILE.pasm).')
        p.add_argument('-M', '--map-file',
                       help='Set map file output (default: OUTPUT-FILE.map).')
        p.add_argument('-s', '--symbol-file',
                       help='Set symbol file output (default: OUTPUT-FILE.sym).')

        p.add_argument('-V', '--verilog', action="store_true",
                       help="""Output verilog .list files. If generating long model images, and the object
                       file addresses span the RAM/ROM divide (&80:0000), two
                       sets of Verilog list images will be generated. The first
                       two will be MSB and LSB list files for low memory (RAM),
                       the other two for ROM.""")

        p.add_argument('--verilog-ram-size', metavar="KWORDS", default=512, type=int,
                       help="""Set the maximum RAM size for Verilog image generation (long model only). The
                       default is meant for faster image generation. (default:
                       %(default)s)""")
        
        p.add_argument('--verilog-rom-size', metavar="KWORDS", default=512, type=int,
                       help="""Set the maximum ROM size for Verilog image generation (long model only). The
                       default is meant for faster image generation. (default:
                       %(default)s)""")

        p.add_argument('--blocksize', metavar="KWORDS", default=1, type=int,
                       help="""Binary files will be generated in sizes that are multiples of this block size,
                       expressed in Kilowords. Specify 0 to disable padding.
                       (default: %(default)s)""")

        # p.add_option('', '--xpm',
        #              help='Output an XPM file mapping the memory image. ' + 
        #              'This is merely a visualisation toy. (default: output nothing).')

        # p.add_option('', '--no-builtins', action='store_true',
        #              help='Start with no built-in symbols at all.')
        # p.add_option('-v', '--verbose', action='store_true',
        #              help='Print out assembled code')
        # p.add_option('', '--min-addr', action='store', type='int', default=0x0000,
        #              help='Set the starting address for dumping objects, verilog output ' + \
        #                  'et cetera (default: 0)')
        # p.add_option('', '--max-addr', action='store', type='int', default=0xffff,
        #              help='Set the ending address for dumping objects, verilog output ' + \
        #                  'et cetera (default: 65535)')
        # p.add_option('-B', '--banked', action='store_true',
        #              help='Assemble for extended 24-bit address space. All addresses are ' + \
        #                  'mod 8192, and you must use the .bank directive at least once.')

        p.add_argument('-p', '--page-warning', action='store', type=int, default=256,
                     help="""Warn when using .page generates more this many slack words. (default: %(default)s)""")

        p.add_argument('-I', '--include', metavar='DIRECTORY', action='append', default=['.'],
                       help="""Add to the include path. Can be specified multiple times to add multiple
                       search directories. Directories will be searched in the
                       order specified. The include path contains only the
                       current directory by default. Files in the current
                       directory will always take precedence.""")

        p.add_argument('--show-builtins', action='store_true',
                       help='Print out built-ins and exit without assembling anything.')

        p.add_argument('--debug-assembly', action='store_true',
                       help="""Output all object code and data writes in a fully pre-processed format that
                       can be fed back to the assembler, if needed. This will echo the same data written to the
                       .pasm file set with --asm-file or -a.""")

        p.add_argument('--debug-lexer', action='store_true',
                       help="""Debug the lexer by printing out its state of the lexer for every parsed
                       character. Extremely verbose.""")

        p.add_argument('--debug-preprocessor', action='store_true',
                       help="""Debug the preprocessor. This will print out includes and macro expansion info.""")

        p.add_argument('--debug-parser', action='store_true',
                       help="""Debug the parser by printing out every line in tokenised form. Extremely verbose.""")

        p.add_argument('--debug-macros', action='store_true',
                       help="""Debug macro definitions and use.""")

        p.add_argument('--debug-pokes', action='store_true',
                       help="""Debug memory image writes.""")

        p.add_argument('-D', '--debug', action='store_true',
                       help="""Raise full exception dumps on unexpected failures.""")

        p.add_argument(metavar="INPUT-FILE", dest="input_files",
                       nargs="*", help="Provide one or more files to assemble.")

        self.args = p.parse_args()


    def setup_address_space(self):
        """Set up the assembler for the specified assembly model."""
        if self.args.model == "short":
            self.memsize = 65536
            self.model = self.MODEL_SHORT
            self.addr_mask = 0xffff
            self.minaddr = self.memsize
            self.maxaddr = 0

        elif self.args.model == "long":
            self.memsize = 16777216
            self.model = self.MODEL_LONG
            self.addr_mask = 0xffffff
            self.minaddr = self.memsize
            self.maxaddr = 0
            # self.boundwarn_low = False
            # self.boundwarn_high = False
            # self.set_addr(0)

        else:
            self.die("Unknown model '{}'".format(self.args.model))

        # The memory image. The multiplication voodoo proves to be a very fast
        # way to initialise an array in Python: about 10 times faster than
        # passing [0] * self.MEMSIZE as the array initialiser.
        self.mem = array.array('H', [0]) * self.memsize
        self.sentinel = array.array('B', [self.PKT_UNUSED]) * self.memsize

        # self.boundwarn_low = False
        # self.boundwarn_high = False
        # self.set_addr(0)


    def run(self):
        """Run the assembler."""

        # Parse the command line
        self.parse_command_line()

        # Set our various output filenames.
        base = os.path.splitext(self.args.out_file)[0]
        self.asm_file = self.args.asm_file or (base + '.pasm')
        self.symbol_file = self.args.symbol_file or (base + '.sym')
        self.map_file = self.args.map_file or (base + '.map')
        # self.xpm_map_file = self.args.xpm or (base + '.xpm')

        # Open the pasm file early.
        try:
            self.pasm = open(self.asm_file, 'wt')
        except Exception as e:
            self.die("unable to open {}: {}".format(self.asm_file, e))

        # Check mode of operation and act on it.
        if self.args.show_builtins:
            print(self.BUILTINS.strip())
            sys.exit(0)

        if not self.args.input_files:
            self.die("no input files")

        # # Sanity checking
        # if self.opts.min_addr > self.opts.max_addr:
        #     self.die('--min-addr must be less than or equal to --max-addr.')
        
        # For everything else, we need a filename to assemble.
        if not self.args:
            self.die('an input file must be provided.')

        # Initialise for assembly
        self.setup_address_space()
        self.ctx = list()
        self.filename = '<none>'
        self.linenum = -1
        self.preprocessor_depth = 0
        self.final_pass = False

        # Make three passes through input files.
        self.pass_macro_discovery()
        self.pass_assembly()
        self.pass_assembly(final_pass=True)

        #pprint.pprint(self.macros, width=200)
        #pprint.pprint(self.symbols, width=200)
        # self.pass_symbol_discovery()
        # self.pass_assembly()

        # Sanity checks
        self.sanity_checks()

        # Output stuff
        self.write_symbol_table()
        self.write_map_file()
        self.write_object()

        self.pasm.close()


    ###############################################################################
    #
    # ERROR REPORTING
    #
    ###############################################################################


    def _msg(self, msg, filename=None, linenum=None):
        """Output a message, along with file and line number info."""
        sys.stderr.write('%s:%d: %s\n' % (filename or self.filename,
                                         linenum or self.linenum,
                                         msg))

    def warn(self, msg):
        """Output a warning message."""
        self._msg('warning: ' + msg)


    def warn_once(self, key, msg):
        """Warn once per job."""
        if key in self.one_off_warnings:
            return
        self.warn(msg)
        self.warn("The warning above will only be shown once.")
        self.one_off_warnings[key] = 1


    def error(self, msg):
        """Output an error message and fail."""
        # Is there a macro/file context we should print out?
        #pprint.pprint(self.ctx)
        for (ctx_filename, ctx_linenumber, ctx_macro) in self.ctx:
            if ctx_macro:
                self._msg("error: (in macro '{}' invocation from {}:{})".format(
                    ctx_macro, ctx_filename, ctx_linenumber))
            else:
                self._msg("error: (included from {}:{})".format(ctx_filename, ctx_linenumber))

        self._msg('error: ' + msg)


    def fail(self, msg):
        """Output an error message and fail."""
        self.error(msg)
        sys.exit(1)


    def die(self, msg):
        """Output an interal, fatal error and fail."""
        sys.stderr.write('%s: %s\n' % (sys.argv[0], msg))
        sys.exit(1)


    ###############################################################################
    #
    # DEBUGGING
    #
    ###############################################################################

    def debug_lexer(self, s):
        if self.args.debug_lexer:
            print("{}:{}: [lexer] {}".format(self.filename, self.linenum, s))
            
    
    def debug_parser(self, s):
        if self.args.debug_parser:
            print("{}:{}: [parser] {}".format(self.filename, self.linenum, s))
            
    
    def debug_preprocessor(self, s):
        if self.args.debug_preprocessor:
            print("{}:{}: [preprocessor] {}".format(self.filename, self.linenum, s))
            
    
    def debug_macros(self, s):
        if self.args.debug_macros:
            print("{}:{}: [macros] {}".format(self.filename, self.linenum, s))


    def debug_assembly(self, s):
        if self.args.debug_assembly:
            print("{}:{}: [assembly] {}".format(self.filename, self.linenum, s))
            
    
    def debug_pokes(self, s):
        if self.args.debug_pokes:
            print("{}:{}: [poke] {}".format(self.filename, self.linenum, s))


    ###############################################################################
    #
    # UTILITIES
    #
    ###############################################################################

    def fmt_addr(self, addr=None):
        """Format the specified address according to the current assembly model. If the
        address is left out, the current assembly address is formatted."""
        if addr is None:
            addr = self.addr

        # TODO: Also format 24-bit addresses
        if self.model == self.MODEL_SHORT:
            return hex(addr)[2:].zfill(4)
        else:
            return hex(addr)[2:].zfill(6)


    def get_nsname(self, symbol=None):
        if symbol is None and self.ns == []:
            return '.'
        elif symbol is None:
            return '.'.join(self.ns)
        return '.'.join(self.ns + [symbol])


    def is_word(self, token, word=None):
        """Return True if the specified token is a bare word. With an optional second
        argument, return True if it's the exact word specified."""
        return token[0] == self.TKT_WORD and ((word is None) or (token[1] == word))


    def is_str(self, token):
        """Return True if the specified token is a string."""
        return token[0] == self.TKT_STR


    def is_label(self, token):
        """Return True if the specified token represents a label."""
        return token[0] == self.TKT_WORD and token[1][-1] == ':'


    def is_directive(self, token):
        """Return True if the specified token represents a label."""
        return token[0] == self.TKT_WORD and token[1][0] == '.'


    def check_valid_directive(self, token):
        """Return the handler method for a token representing a dot
        directive. If the token is not a valid dot directive, fail the
        parse."""
        if type(token) == tuple:
            token = token[1]
        try:
            return self.DIRECTIVES[token]
        except KeyError:
            self.fail("Unknown directive {}.".format(token))


    def get_symbol(self, sym):
        """Return the value of a symbol from the symbol table. Return None if it
        couldn't be found.
        """
        for x in range(len(self.ns), -1, -1):
            try:
                nssym = '.'.join(self.ns[0:x] + [sym])
                #print "Pass", self.passnum, ", Looking up", nssym, self.ns
                return self.symbols[nssym]
            except KeyError:
                continue
        return None


    def add_symbol(self, nssym, symbol_object):
        """Add a symbol to the symbol table. Argument sym should be a symbol name fully
        qualified with its namespace; symbol_object should be an instance of a
        class derived from class Symbol.
        """
        existing = self.symbols.get(nssym)
        if existing is not None:
            if self.final_pass and existing.resolved and symbol_object.resolved and \
               existing.val != symbol_object.val:
                self.warn("redefining symbol {} (defined on {}:{}) from &{:>04x} to &{:>04x}".format(
                    nssym, existing.filename, existing.linenum, existing.val, symbol_object.val))

        self.symbols[nssym] = symbol_object

        
    def get_macro(self, sym):
        """Return the definition tuple of a macro. Return None if it couldn't
        be found.
        """
        for x in range(len(self.ns), -1, -1):
            try:
                nssym = '.'.join(self.ns[0:x] + [sym])
                return self.macros[nssym]
            except KeyError:
                continue
        return None


    # def push_parse_context(self, macro, filename, linenum, mdef):
    #     """Push a macro context onto the context stack. This is used for reporting the
    #     exact location of file inclusions and macro expansions.

    #     This is used for error reporting.

    #     """
    #     self.ctx.append((macro, filename, linenum, mdef))


    # def pop_parse_context(self):
    #     """Pop a macro context from the context stack.

    #     This is used for error reporting."""
    #     try:
    #         return self.ctx.pop()
    #     except IndexError:
    #         self.die('Attempted pop from empty context stack.')


    def enter_parse_context(self, filename, linenum, macro):
        """Push a macro context onto the context stack. This is used for reporting the
        exact location of file inclusions and macro expansions.

        This is used for error reporting.

        """
        self.ctx.append((self.filename, self.linenum, self.current_macro))
        self.filename = filename
        self.linenum = linenum
        self.current_macro = macro


    def leave_parse_context(self):
        """Pop a macro context from the context stack.

        This is used for error reporting."""
        try:
            (self.filename, self.linenum, self.current_macro) = self.ctx.pop()
        except IndexError:
            self.die('Attempted pop from empty context stack.')


    def set_addr(self, a):
        """Set the assembly address."""
        self.addr = a & self.addr_mask
        self.jumped = True
        self.wrap = False
        if a > self.memsize:
            self.warn_once("memsize_warning",
                           "Address &{:>x} is beyond the maximum address for this model (&{})".format(
                               a, self.fmt_addr(self.memsize - 1)))


    def inc_addr(self, by=1):
        """Increment the address."""
        # If the address has just been set and no data has been written to
        # memory, don't increment yet.
        if self.jumped:
            return

        self.addr = (self.addr + by) & self.addr_mask
        if self.addr == 0:
            self.wrap = True
        elif self.wrap and self.addr == 1:
            # Warn the second time this gets here to avoid a spurious
            # warning when 0xffff is written to (and no further).
            self.wrap = False
            self.warn('64KW bank boundary crossed.')


    
            
    
    ###############################################################################
    #
    # THE TOKENISER
    #
    ###############################################################################

    def lexer(self, iterable):
        """Lex an iterable containing lines. This could be a file-like object or
        iterable of strings, one string per line. Yield lists of tuples
        (token_type, string) for each lexed component of the line.

        """
        fmt = lambda x: x.replace('{', '\033[1m{').replace('}', '}\033[0m')

        whitespace = "\t\r\n "
        comments = ";/"

        for i, line in enumerate(iterable, 1):
            self.line = line
            self.linenum += 1
            line = line.strip()
            tokens = []
            tmptoken = ''
            state = self.TOK_SP
            self.debug_lexer(fmt("\n+++ LINE: '{}'").format(line))
            for c in line.rstrip('\n'):
                self.debug_lexer(fmt("*** c='{c}', state={state}, tmptoken={tmptoken}, tokens={tokens}").format(**locals()))

                # The state machine (warning: sprawling code!)
                if state == self.TOK_SP:
                    # Ignore white space, look for the beginning of a word
                    if c in whitespace:
                        continue
                    elif c in comments:
                        # Comment. Ignore rest of line.
                        break
                    elif c == '"':
                        state = self.TOK_QUOTE
                        tmptoken = ''
                    elif c == '(':
                        tokens.append((self.TKT_LPAREN, '('))
                        state = self.TOK_PAREN
                    elif c == '@':
                        tmptoken = "@"
                        state = self.TOK_ATEXPR
                    elif c == ':':
                        self.fail("Parse error. If this is label, delete white space before the colon.")
                    else:
                        state = self.TOK_WORD
                        tmptoken = c

                elif state == self.TOK_QUOTE:
                    # Read until a closing double quote is found
                    if c == '\\':
                        state = self.TOK_QUOTEESC
                        continue
                    elif c == '"':
                        state = self.TOK_SP
                        tokens.append((self.TKT_STR, tmptoken))
                    else:
                        tmptoken += c

                elif state == self.TOK_QUOTEESC:
                    # An escape character has been seen in a string. Some
                    # characters (e.g \n) are treated specially. All others are
                    # inserted into the string as-is.
                    tmptoken += self.ESCAPED_CHARS.get(c, c)
                    state = self.TOK_QUOTE

                elif state == self.TOK_ATEXPR:
                    # An @-expression has been seen.
                    if c in whitespace:
                        tokens.append((self.TKT_WORD, tmptoken))
                        state = self.TOK_SP
                    # elif c in comments:
                    elif c == ';':
                        # Start of comment. Ignore rest of line.
                        if tmptoken:
                            tokens.append((self.TKT_WORD, tmptoken))
                        break
                    elif c == '{':
                        # Transition to a long @-expression. Keep tokens. Don't
                        # bother with the '{' though.
                        state = self.TOK_ATEXPRLONG
                    else:
                        tmptoken += c

                elif state == self.TOK_ATEXPRLONG:
                    # An @-expression has been seen.
                    if c == '}':
                        # Done with expression. Don't bother adding the trailing }.
                        tokens.append((self.TKT_WORD, tmptoken))
                        state = self.TOK_SP
                    else:
                        tmptoken += c

                elif state == self.TOK_WORD:
                    # Add letters to a word until we meet whitespace
                    if c in whitespace:
                        tokens.append((self.TKT_WORD, tmptoken))
                        state = self.TOK_SP
                    elif c in comments:
                        # Start of comment. Ignore rest of line.
                        if tmptoken:
                            tokens.append((self.TKT_WORD, tmptoken))
                        break
                    elif c == '(':
                        if tmptoken:
                            tokens.append((self.TKT_WORD, tmptoken))
                            tmptoken = ''
                        tokens.append((self.TKT_LPAREN, '('))
                        state = self.TOK_PAREN
                    else:
                        # We'll allow any character here.
                        tmptoken += c

                elif state == self.TOK_PAREN:
                    # We just start an arg list.
                    if c in whitespace:
                        continue
                    elif c in comments:
                        # Start of comment. Ignore rest of line.
                        if tmptoken:
                            tokens.append((self.TKT_PARENWORD, tmptoken))
                        break
                    elif c in ')':
                        # End of arg list
                        tokens.append((self.TKT_RPAREN, ')'))
                        state = self.TOK_SP
                    elif c == ',':
                        self.fail("Unexpected comma in parameter list.")
                    else:
                        state = self.TOK_PARENWORD
                        tmptoken = c

                elif state == self.TOK_PARENWORD:
                    # im in ur parens lists parsing ur words
                    if c in whitespace:
                        # Allow multiple tokens to be provided as a single
                        # argument, separated by spaces.
                        if tmptoken:
                            tokens.append((self.TKT_WORD, tmptoken))
                            tmptoken = ''
                    elif c == ',':
                        if not tmptoken:
                            self.fail("Unexpected comma.")
                        tokens.append((self.TKT_WORD, tmptoken))
                        tokens.append((self.TKT_ARGSEP, ','))
                        state = self.TOK_PAREN
                    elif c == ')':
                        if not tmptoken:
                            self.fail("Unexpected closing parenthesis.")
                        tokens.append((self.TKT_WORD, tmptoken))
                        tokens.append((self.TKT_RPAREN, ')'))
                        state = self.TOK_SP
                    else:
                        # We'll allow any character here.
                        tmptoken += c

            # Finish lexing the last parsed word, if there is one. Ending a
            # line in some states is a parse error. Handle those too.
            if state in (self.TOK_WORD, self.TOK_ATEXPR) and tmptoken:
                tokens.append((self.TKT_WORD, tmptoken))
            elif state == self.TOK_QUOTEESC:
                self.fail("Line ended in backspace. Were you escaping something?")
            elif state == self.TOK_QUOTE:
                self.fail("Missing closing double quote.")
            elif state == self.TOK_ATEXPRLONG:
                self.fail("Missing closing brace while parsing @-expression.")
            elif state != self.TOK_SP:
                self.fail("Parse error")

            # Done.
            if tokens:
                yield(tokens)

            # Next line.
        # End of parse
    # End of laxer


    ###############################################################################
    #
    # THE PREPROCESSOR
    #
    ###############################################################################

    def preprocess_file(self, filename):
        try:
            f = open(filename, 'rt')
        except Exception as e:
            self.die("Unable to open {}: {}".format(filename, e))

        # Set the parse context.
        self.filename = filename
        self.linenum = 0
        self.debug_lexer("\n--- START FILENAME: {}'".format(filename))
        for tokens in self.preprocessor(f):
            yield tokens
        self.debug_lexer("\n--- END FILENAME: {}'".format(filename))


    def preprocessor(self, line_iterable):
        """A very basic preprocessor: just handle file inclusions."""
        for tokens in self.lexer(line_iterable):
            self.debug_parser("*** tokens: {}".format(tokens))

            # Is this an include directive?
            if tokens[0] == (self.TKT_WORD, '.include'):
                for tokens in self.preprocessor_dot_include(tokens):
                    yield tokens
                continue

            # Is there what appears to be a macro call?
            elif self.expand_macros and \
               (self.TKT_WORD, '.macro') not in tokens and \
               (self.TKT_LPAREN, '(') in tokens:
                for tokens in self.preprocessor_expand_macro(tokens):
                    yield tokens

            # Nope, just yield the tokenised line
            else:
                yield tokens


    def preprocessor_dot_include(self, tokens):
        """Include an .asm file."""

        if self.preprocessor_depth >= self.MAX_PREPROCESSOR_DEPTH:
            self.fail("Too many nested includes or macro calls.")
        self.preprocessor_depth += 1

        try:
            include, include_fname = tokens
            assert include[0] == self.TKT_WORD # Sanity check; this has been checked already
            assert include_fname[0] == self.TKT_STR

        except:
            self.fail("Error parsing .include directive.")

        # Look for the file in the specified include path
        fname = include_fname[1]
        class FoundFile(Exception):
            pass

        try:
            for pathcomp in self.args.include:
                fname = os.path.join(pathcomp, include_fname[1])
                if os.path.exists(fname):
                    raise FoundFile
            self.fail("File {} not found in include path {}".format(
                include_fname[1], ':'.join(self.args.include)))
        except FoundFile:
            pass

        # Get a parse state and return it to the preprocessor
        self.debug_preprocessor("include file {} ({})".format(include_fname[1], fname))
        self.enter_parse_context(self.filename, self.linenum, None)

        # Note: mutual recursion!
        for tokens in self.preprocess_file(fname):
            yield tokens

        # Done, pop the parse context
        self.leave_parse_context()
        self.debug_preprocessor("end of included file {}".format(fname))
        self.preprocessor_depth -= 1


    def preprocessor_expand_macro(self, tokens):
        """Expand the macro call in this tokenised line."""

        # Use the include depth to make sure we're not in some deep
        # mutual macro recursion.
        if self.preprocessor_depth >= self.MAX_PREPROCESSOR_DEPTH:
            self.fail("Too many nested includes or macro calls.")
        self.preprocessor_depth += 1

        # There may be a label at the start of this line. If so, yield
        # it as a line of its own and we'll be processing the rest of
        # the line here.
        if self.is_label(tokens[0]):
            yield [tokens[0]]
            tokens = tokens[1:]

        # The syntax for the rest of the line should be: macro name,
        # lparen, zero or more args, rparen.
        macro_name = tokens[0][1]
        if not self.is_word(tokens[0]):
            self.fail("Macro name expected")
        elif tokens[1] != (self.TKT_LPAREN, '('):
            self.fail("Left parenthesis expected")
        elif tokens[-1] != (self.TKT_RPAREN, ')'):
            # This is mostly a sanity check, the lexer/tokeniser has already
            # taken care of this.
            self.fail("Right parenthesis expected")

        # Now we know that tokens[2:-1] contain the argument list. Form them into strings
        #arglist = tokens[2:-1]
        if tokens[2:-1]:
            arglist, argnum = [[]], 0
            for token in tokens[2:-1]:
                if token == (self.TKT_ARGSEP, ','):
                    argnum += 1
                    arglist.append([])
                else:
                    arglist[argnum].append(token[1])
        else:
            arglist = []

        self.debug_macros("Macro call {} tokenlist={} split_arglist={}".format(
            macro_name, tokens[2:-1], arglist))

        # Get the macro definition.
        macro = self.get_macro(macro_name)
        if macro is None:
            self.fail("Undefined macro '{}'".format(macro_name))

        #pprint.pprint(macro, width=200)
        macro_filename, macro_start, macro_args, macro_def = macro
        self.debug_macros("Macro call {}: defined in {}:{}, args={}, def={}".format(
            macro_name, macro_filename, macro_start, macro_args, repr(macro_def)))

        if len(macro_args) != len(arglist):
            self.fail("macro {} (definition at {}:{}) expects {} arguments ({}) but {} given".format(
                macro_name, macro_filename, macro_start,
                len(macro_args), ', '.join(macro_args), len(arglist)))

        # Expand arguments
        macro_exp = macro_def
        for arg_name, arg_valuelist in zip(macro_args, arglist):
            arg_value = ' '.join(arg_valuelist)
            self.debug_macros("Macro call {}: expand %{} to {}".format(macro_name, arg_name, arg_value))
            macro_exp = re.sub('%' + arg_name + r'\b', arg_value, macro_exp)

        self.debug_macros("Macro call {}: expanded to '{}'".format(macro_name, repr(macro_exp)))

        # Now pretend we're including a very small file in this
        # location. Preprocess it and yield tokens.
        #savedState = self.filename, self.linenum # Why is this necessary?
        self.enter_parse_context(macro_filename, macro_start, macro_name)

        # Note: mutual recursion!
        for tokens in self.preprocessor(macro_exp.split('\n')):
            yield tokens

        # Done, pop the parse context
        self.leave_parse_context()
        #self.filename, self.linenum = savedState
        self.debug_preprocessor("Macro call {}: end of macro expansion".format(macro_name))
        self.preprocessor_depth -= 1



        # try:
        #     include, include_fname = tokens
        #     assert include[0] == self.TKT_WORD # Sanity check; this has been checked already
        #     assert include_fname[0] == self.TKT_STR

        # except:
        #     self.fail("Error parsing .include directive.")

        # # Look for the file in the specified include path
        # fname = include_fname[1]
        # class FoundFile(Exception):
        #     pass

        # try:
        #     for pathcomp in self.args.include:
        #         fname = os.path.join(pathcomp, include_fname[1])
        #         if os.path.exists(fname):
        #             raise FoundFile
        #     self.fail("File {} not found in include path {}".format(
        #         include_fname[1], ':'.join(self.args.include)))
        # except FoundFile:
        #     pass

        # # Get a parse state and return it to the preprocessor
        # self.debug_preprocessor("include file {} ({})".format(include_fname[1], fname))
        # savedState = self.filename, self.linenum # Why is this necessary?
        # self.pushParseContext(fname, self.filename, self.linenum, None)

        # # Note: mutual recursion!
        # for tokens in self.preprocessor(fname):
        #     yield tokens

        # # Done, pop the parse context
        # self.popParseContext()
        # self.filename, self.linenum = savedState
        # self.debug_preprocessor("end of included file {}".format(fname))
        self.preprocessor_depth -= 1


    ###############################################################################
    #
    # PARSING TOKENS INTO VALUES
    #
    ###############################################################################

    def parse_expression(self, token):
        """Parse an @-expression."""
        self.debug_lexer("@-Expression '{}'".format(token))
        if token == '@':
            return self.addr & 0xffff

        # CFTASM3 only supported simple expressions. We'll cover those for now
        # and parse using a regexp.

        m = re.match('@\s*(?:(\S+?)\s*)?(\+|-|\*|/|\||&|\^|<<|>>)\s*(\S+?)\s*$', token)
        if not m:
            self.fail("Invalid @-expression {}".format(token))
        else:
            try:
                tok_left, op, tok_right = m.groups()
                self.debug_lexer("@-Expression '{}', left='{}', op='{}', right='{}'".format(token, tok_left, op, tok_right))

                # Parse the left operand (which may be missing)
                if tok_left is None or tok_left == '':
                    left = self.addr
                else:
                    lparsed = list(self.parse_fields([[ self.TKT_WORD, tok_left ]], masked=False))
                    if len(lparsed) != 1:
                        self.fail("sanity check, expected a single lparsed result, not {}".format(lparsed))
                    left = lparsed[0]

                # Parse the right operand
                rparsed = list(self.parse_fields([[ self.TKT_WORD, tok_right ]], masked=False))
                if len(rparsed) != 1:
                    self.fail("sanity check, expected a single rparsed result, not {}".format(rparsed))
                right = rparsed[0]

                # Apply the operator
                val = {
                    '+': lambda a, b: a + b,
                    '-': lambda a, b: a - b,
                    '|': lambda a, b: a | b,
                    '&': lambda a, b: a & b,
                    '*': lambda a, b: a * b,
                    '/': lambda a, b: a // b,
                    '^': lambda a, b: a ^ b,
                    '<<': lambda a, b: a << b,
                    '>>': lambda a, b: a >> b,
                }[op](left, right)
                
                return val & 0xffff
                
            except Exception as e:
                self.fail("Parse error.")


    def parse_literal(self, literal):
        """Parse an integer literal.
    
        Base Conventions and sigils:
    
        * default base: 10 (e.g. 64, 0001). No sigil for this one.
        * base 16: "&" sigil. E.g. &20 == 32
        * base 2: "#. E.g. #10:1100_ = 88. Ony of "0-._" are parsed as '0'. '1'
          is parsed as 1. ":,'" are ignored entirely. Useful for building
          human-readable bitfields.

        >>> x = CFTAssembler()
        >>> x.parse_literal('&10')
        16
        >>> x.parse_literal('&fffe')
        65534
        >>> x.parse_literal('16')
        16
        >>> x.parse_literal('#10000')
        16
        >>> x.parse_literal('#1____')
        16
        >>> x.parse_literal('#1....')
        16
        >>> x.parse_literal('#1----')
        16
        >>> x.parse_literal('#1--:--')
        16
        >>> x.parse_literal('foobar')
        Traceback (most recent call last):
        ValueError: unable to parse literal 'foobar'

        """
        if literal.startswith('#'):
            try:
                newlit = ''.join(self.BINDICT[x] for x in literal[1:])
                newint = int(newlit, 2)
            except:
                self.fail("Unable to parse binary literal '{}'".format(literal))
            #print("lit={}, conv={}".format(newlit, newint))
            return int(newlit, 2)

        elif literal.startswith('&'):
            try:
                return int(literal[1:], 16)
            except:
                self.fail("Unable to parse hexadecimal literal '{}'".format(literal))

        elif literal.startswith('0x'):
            try:
                return int(literal[2:], 16)
            except:
                self.fail("Unable to parse hexadecimal literal '{}'".format(literal))

        elif literal[0] in "0123456789-":
            try:
                return int(literal)
            except:
                self.fail("Unable to parse decimal literal '{}'".format(literal))

        return None
        

    def parse_fields(self, tokens, masked=None, strings_too=False):
        """Parse tokens into numbers and return them. Tokens can be numeric literals in
        binary, decimal or hexadecimal, symbols that are resolved into numbers,
        symbolic @-expressions, or (if strings_too=True is specified)
        codepoints of individual characters in a string. The parsed fields are
        yielded to the caller.

        If this is the final assembly pass, unresolved symbols cause a fatal error.
        """
        assert masked is not None, "for sanity, always specify masked=True or masked=False"
        self.debug_parser("Parse tokens: {}".format(tokens))
        for tkt, token in tokens:
            self.debug_parser("Parse token: ({},{})".format(tkt, token))
            if tkt == self.TKT_WORD:
                if token[0] == '@':
                    self.debug_parser("Parse expression: {}".format(token))
                    parsed_literal = self.parse_expression(token)
                else:
                    self.debug_parser("Parse literal: {}".format(token))
                    parsed_literal = self.parse_literal(token)
                if parsed_literal is not None:
                    self.debug_parser("Yield as-is: {} -> {}".format(token, parsed_literal))
                    if masked:
                        yield parsed_literal & 0x3ff
                    else:
                        yield parsed_literal
                else:
                    # Do we have a resolved symbol in the symbol table?
                    sym = self.get_symbol(token)
                    if sym is not None and sym.val is not None:
                        self.debug_parser("Looked up symbol {}: value {}".format(token, sym))
                        if masked:
                            yield sym.masked_val
                        else:
                            yield sym.val
                    else:
                        self.debug_parser("Looked up symbol {}: not found".format(token))
                        if self.final_pass:
                            self.fail("Undefined symbol {}".format(token))
                        yield token

            elif tkt == self.TKT_STR:
                if not strings_too:
                    self.fail("Strings are not allowed here")
                for c in token:
                    yield(ord(c))

        self.debug_parser("End of token parse.")


    def assemble_single_field(self, tokens, masked=False):
        value = 0
        for token in self.parse_fields(tokens, masked=masked):
            if type(token) != int:
                # In the final pass, parse_fields() throws a parse error on
                # undefined fields. So we're safe to do this here.
                return None
            else:
                value = value | token
        return value & 0xffff
        

    ###############################################################################
    #
    # THE PARSER
    #
    ###############################################################################


    def prepare_for_parse(self):
        """Initialise parse state."""
        # Clear our parse state
        self.ns = []              # The namespace stack
        self.scope = 0            # The scope number counter
        self.scopedepth = 0       # Scope depth (for sanity checking)
        self.longstring = None    # This holds long strings
        self.compstring = None    # This holds compressed strings
        self.jumped = False       # The address has changed, don't increment.
        self.set_addr(0)
        self.current_macro = None


    def deprecated_directive(self, tokens):
        """Handler for deprecated directives. Just issues a warning."""
        if len(tokens) < 1:
            self.fail("sanity check failed.")
        self.warn("directive {} is deprecated.".format(tokens[0][1]))


    def parse_dot_pushns(self, tokens):
        if len(tokens) != 2:
            self.fail("incorrect use of .pushns")
        elif not self.is_word(tokens[1]):
            self.fail("expected namespace identifier after .pushns")
        elif tokens[1][1][0] == '_':
            self.fail("namespace names may not start with _")
        else:
            self.ns.append(tokens[1][1])
            self.debug_parser("Namespace set to {}".format(self.get_nsname()))


    def parse_dot_popns(self, tokens):
        if not self.ns:
            self.fail(".popns seen before .pushns")
        elif len(tokens) == 1:
            # Only warn about this on the first pass
            if self.expand_macros == False:
                self.warn("missing namespace to pop, assuming {}. (this will be an error soon)".format(self.ns[-1]))
            self.ns.pop(-1)
            self.debug_parser("Namespace popped back to {}".format(self.get_nsname()))
        elif not self.is_word(tokens[1]):
            self.fail("expected namespace identifier after .popns")
        elif tokens[1][1][0] == '_':
            self.fail("namespace names may not start with _")
        elif self.ns[-1].startswith("_s"):
            self.fail("missing .endscope before .popns")
        elif tokens[1][1] != self.ns[-1]:
            self.fail(".popns {} seen but last namespace pushed was {}".format(tokens[1][1], self.ns[-1]))
        else:
            self.ns.pop(-1)
            self.debug_parser("Namespace popped back to {}".format(self.get_nsname()))


    def parse_dot_scope(self, tokens):
        if len(tokens) != 1:
            self.fail("error parsing .scope, no arguments expected")
        else:
            self.scope += 1
            self.scopedepth += 1
            self.ns.append("_s" + str(self.scope))
            self.debug_parser(".scope {}, depth {}".format(self.scope, self.scopedepth))


    def parse_dot_endscope(self, tokens):
        if not self.ns:
                self.fail(".endscope seen before .scope")
        elif len(tokens) != 1:
            self.fail("error parsing .endscope, no arguments expected")
        else:
            if not self.ns[-1].startswith("_s"):
                self.fail(".endscope seen before .scope (depth={})".format(self.scopedepth))
            self.ns.pop(-1)
            self.scopedepth -= 1
            if (self.scopedepth < 0):
                self.fail("internal error, scope level is negative")
        self.debug_parser(".endscope {}, depth {}, ns={}".format(self.scope, self.scopedepth, self.ns))


    def parse_dot_equ(self, tokens):
        if len(tokens) < 3:
            self.fail("error parsing .equ, expecting at least two arguments.")
        if not all(self.is_word(x) for x in tokens[1:]):
            self.fail("error parsing .equ, expecting bareword.")

        sym, definition = tokens[1][1], list(x[1] for x in tokens[2:])
        nssym = self.get_nsname(sym)

        parsed = list(self.parse_fields(tokens[2:], masked=False))

        # Has this symbol been resolved fully at this time?
        if all(type(x) == int for x in parsed):
            value = parsed[0]
            for x in parsed[1:]:
                value = value | x
            self.add_symbol(nssym, Symbol(self.filename, self.linenum, nssym, val=value, tokens=" ".join(definition)))
        else:
            self.add_symbol(nssym, Symbol(self.filename, self.linenum, nssym, tokens=parsed))
        self.debug_parser(".equ seen: sym={}, nssym={}, def={}, parsed={}".format(sym, nssym, definition, parsed))


    def parse_dot_fill(self, tokens):
        directive = tokens[0][1]
        if len(tokens) < 3:
            self.fail("error parsing {}, expecting at least two arguments.".format(directive))
        if not self.is_word(tokens[1]):
            self.fail("{} expects a positive number as its first argument.".format(directive))

        if directive  == '.regfill':
            pkt = self.PKT_REG
        else:
            pkt = self.PKT_FILL

        count = self.parse_literal(tokens[1][1])
        if not count or count < 1:
            self.fail("{} expects a positive number as its first argument.".format(directive))
        if count > 65535:
            self.warn("fill count doesn't fit in 16 bits. (CFT-native assembler will balk)")

        addr = self.addr
        value = self.assemble_single_field(tokens[2:], masked=False)

        if value is None:
            if self.final_pass:
                self.fail("failed to assemble fill value. '{}'".format(' '.join(x[1] for x in tokens[2:])))
        else:
            self.debug_assembly("{} directive: count={} value=&{:>04x}".format(directive, count, value))

        # Quick sanity check
        if any(x != self.PKT_UNUSED for x in self.sentinel[addr:(addr+count)]):
            self.warn("{} overwrites previously assembled data.".format(directive))

        # Don't bother poking, just assign to the memory directly. It's much
        # faster. Warning: voodoo! Also, we only do this on the final pass.
        if self.final_pass:
            self.mem[addr:(addr + count)] = array.array('H', [value]) * count
            self.sentinel[addr:(addr + count)] = array.array('B', [pkt]) * count

            for n in range(count):
                self.logasm(addr + n, value, orgline=self.line)

        self.addr += count

        if self.addr > self.memsize:
            self.fail("Filled past end of memory space.")

        self.minaddr = min(self.minaddr, addr)
        self.maxaddr = max(self.maxaddr, self.addr - 1)


    def parse_dot_macro(self, tokens, macro_def):
        # Check it
        if macro_def is not None:
            self.fail("nested .macro definition. Probably a missing .endmacro.")
        elif tokens[1][0] != self.TKT_WORD:
            self.fail("Expected macro name")
        elif tokens[2] != (self.TKT_LPAREN, '('):
            self.fail("Expected macro definition argument list")
        elif tokens[-1] != (self.TKT_RPAREN, ')'):
            self.fail("Parse error reading macro definition: garbage after argument list")
        else:
            # Ensure all the args are the right type.
            for arg in tokens[3:-2]:
                if arg[0] not in (self.TKT_WORD, self.TKT_ARGSEP):
                    self.fail("Parse error reading macro argument '{}'. Word expected.".format(arg[1]))
        macro_def = ''
        macro_start = self.linenum
        macro_name = tokens[1][1]
        macro_args = list(x[1] for x in tokens[3:-1] if x != (self.TKT_ARGSEP, ','))
        self.debug_macros("defined macro name={}, args={}, start={}".format(macro_name, macro_args, macro_start))
        return macro_name, macro_args, macro_start, macro_def


    def parse_dot_endmacro(self, tokens, macro_name, macro_args, macro_start, macro_def):
        if macro_def is None:
            self.fail(".endmacro seen before .macro")
        self.debug_macros(".endmacro seen")
        nsname = self.get_nsname(macro_name)
        ext = self.macros.get(nsname)
        if ext:
            ext = self.macros[nsname]
            self.fail("redefining macro {} (defined in {}:{})".format(nsname, ext[0], ext[1]))
        #print("***", nsname, (self.filename, macro_start, macro_args, macro_def))
        self.macros[nsname] = (self.filename, macro_start, macro_args, macro_def)


    def parse_dot_data(self, tokens):
        if len(tokens) < 2:
            self.fail(".data expects at least one data field.")

        if self.is_word(tokens[0], ".data"):
            pkt = self.PKT_DATA
        elif self.is_word(tokens[0], ".str"):
            pkt = self.PKT_STR
        else:
            self.fail("Unhandled directive {}".tokens[0][1])

        fields = tokens[1:]
        for token in self.parse_fields(fields, masked=False,
                                       strings_too=pkt == self.PKT_STR):
            # Sanity check (parse_fields takes care of better error reporting)
            if type(token) != int:
                assert not self.final_pass or type(token) == int, "undefined symbol"
                self.poke(0xdead, poketype=self.PKT_UNRESOLVED)
                self.inc_addr()
            else:
                self.poke(token, poketype=pkt)
                self.inc_addr()


    def parse_dot_longstring(self, tokens):
        if self.longstring is not None:
            self.fail(".longstring seen before last .longstring was complete.")
        self.longstring = []

        # The string can optionally start on the same line as .longstring, so
        # process it.
        if len(tokens) > 1:
            self.parse_dot_strp(tokens)


    def parse_dot_page(self, tokens):
        """Assemble a page directive.

        Syntax: .page [WORD]

        The .page directive automatically fills memory with WORD (which can be
        any parseable assembler expression, and defaults to zero) until the end
        of the current page is reached. This is useful for keeping code with
        relative addressing in the same memory area.

        Using .page allows code to run without rearranging during development.

        If more than 16 words are filled as a result of .page, a message is
        output during assembly. If more than 64 words are filled, a warning is
        output. If more than 128 words are filled, an error is raised.
        """
        self.jumped = False
        nextpage = (self.addr + 0x400) & 0xfc00
        slack = nextpage - self.addr
        self.addr = nextpage
        self.debug_assembly(".page transition: &{:>04x} to &{:>04x}, {} slack word(s)".format(self.addr, nextpage, slack))
        if slack == 0:
            # Already at the beginning of a page. Do nothing.
            return
        elif slack >= self.args.page_warning:
            if self.final_pass:
                self.warn(".page generated {} word(s) of slack space.".format(slack))


    def parse_dot_strp(self, tokens):
        if len(tokens) < 2:
            self.fail("{} expects at least one string field.".format(tokens[0][1]))

        s = list(self.parse_fields(tokens[1:], masked=False, strings_too=True))

        if self.longstring is not None:
            self.longstring += s
        else:
            self.poke_packed_string(s)


    def parse_dot_endstring(self, tokens):
        if self.longstring is None:
            self.fail(".endstring seen before .longstring.")
        self.poke_packed_string(self.longstring)
        self.longstring = None


    def parse_dot_reg(self, tokens):
        if len(tokens) < 3:
            self.fail("error parsing .reg, expecting at least two arguments.")
        if not all(self.is_word(x) for x in tokens[1:]):
            self.fail("error parsing .reg, expecting bareword.")

        sym, definition = tokens[1][1], list(x[1] for x in tokens[2:])
        nssym = self.get_nsname(sym)

        parsed = list(self.parse_fields(tokens[2:], masked=False))

        # Has this symbol been resolved fully at this time?
        if all(type(x) == int for x in parsed):
            value = parsed[0]
            for x in parsed[1:]:
                value = value | x
            self.add_symbol(nssym, Register(self.filename, self.linenum, nssym, val=value, tokens=" ".join(definition)))
        else:
            self.add_symbol(nssym, Register(self.filename, self.linenum, nssym, tokens=parsed))
        self.debug_parser(".reg seen: sym={}, nssym={}, def={}, parsed={}".format(sym, nssym, definition, parsed))

        # Unlike .equ, a .reg will also step the address and mark the memory
        # map, but like .equ, it doesn't store anything to the object file.
        self.sentinel[self.addr] = self.PKT_REG
        #self.poke(0xdead, poketype=self.PKT_REG)
        self.inc_addr()
        

    def parse_dot_word(self, tokens):
        """Parse a .word directive. Multiple fields are allowed, and they are all ORred
        into a single 16-bit value.
        """
        if len(tokens) < 2:
            self.fail(".word expects at least one data field.")

        pkt = self.PKT_WORD

        value = self.assemble_single_field(tokens[1:], masked=False)
        if value is None:
            # One or more values are undefined. In the final pass, a parse
            # error will be thrown and we never get here.
            self.poke(0xdead, poketype=self.PKT_UNRESOLVED)
            self.inc_addr()
        else:
            self.poke(value, poketype=pkt)
            self.inc_addr()


    def maybe_parse_label(self, tokens):
        """Parse a label or address definition."""

        # Bail out early if there's no label here.
        if not self.is_label(tokens[0]):
            return tokens

        # Consume the first token
        token = tokens.pop(0)
        no_colon = token[1].rstrip(':')

        # Try to parse the specification. We don't use parse_fields because label syntax is very specific.
        numeric_val = self.parse_literal(no_colon)
        if numeric_val is not None:
            self.set_addr(numeric_val)
            self.debug_parser("Address {}".format(self.fmt_addr()))
        else:
            nssym = self.get_nsname(no_colon)
            self.add_symbol(nssym, Label(self.filename, self.linenum, nssym, val=self.addr))
            self.debug_parser("Label {} = {} ({})".format(no_colon, self.fmt_addr(), nssym))

        # Return the rest of the tokens
        return tokens


    def pass_macro_discovery(self):
        self.dots = dict()
        self.prepare_for_parse()
        self.expand_macros = False # Don't expand any macros here
        macro_def = None
        for filename in [ None ] + self.args.input_files:
            if filename is None:
                self.filename = '<builtins>'
                token_generator = self.preprocessor(self.BUILTINS.split('\n'))
            else:
                token_generator = self.preprocess_file(filename)
            
            for tokens in token_generator:
                #self.debug_parser(tokens)

                # Always allow a label as the first token.
                tokens = self.maybe_parse_label(tokens)

                # Nothing else left on this line
                if not tokens:
                    continue

                # Parse namespaces. We need those to qualify any macro names we
                # encounter.
                if self.is_word(tokens[0], ".pushns"):
                    self.parse_dot_pushns(tokens)

                elif self.is_word(tokens[0], ".popns"):
                    self.parse_dot_popns(tokens)

                elif self.is_word(tokens[0], ".scope"):
                    self.parse_dot_scope(tokens)
                    self.debug_parser("scope now {}".format(self.ns[-1]))

                elif self.is_word(tokens[0], ".endscope"):
                    self.debug_parser("endscope {}".format(self.ns[-1]))
                    self.parse_dot_endscope(tokens)

                # Opportunistically parse .equs too, while we're at it
                elif self.is_word(tokens[0], ".equ"):
                    self.parse_dot_equ(tokens)

                # Parse a macro definition
                elif tokens[0] == (self.TKT_WORD, ".macro"):
                    macro_name, macro_args, macro_start, macro_def = self.parse_dot_macro(tokens, macro_def)

                elif tokens[0] in [ (self.TKT_WORD, '.end'), (self.TKT_WORD, '.endmacro') ]:
                    if tokens[0][1] == '.end':
                        self.warn(".end is deprecated. Change to .endmacro.")
                    #print("***", tokens, macro_name, macro_args, macro_start, macro_def)
                    self.parse_dot_endmacro(tokens, macro_name, macro_args, macro_start, macro_def)
                    macro_def = None

                elif macro_def is not None:
                    # Append another line to the macro definition.
                    macro_def += self.line

                # We ignore all other directives and assembly code in this pass.


            # Sanity checks at end of file.
            if macro_def is not None:
                self.fail("File ended before .endmacro was seen.")
            if self.ns and self.ns[-1].startswith("_ns"):
                self.fail("File ended before .endscope was seen.")


    def pass_assembly(self, final_pass=False):
        """Perform an Assembly pass. In this pass, macros are expanded, but labels and
        .equs are still discovered if needded. If final_pass=True, this is the
        final pass and all symbols are expected to be fully resolved into
        values.
        """
        
        self.dots = dict()
        self.prepare_for_parse()
        self.expand_macros = True
        self.final_pass = final_pass

        macro_def = None
        for filename in [ None ] + self.args.input_files:
            if filename is None:
                self.filename = '<builtins>'
                token_generator = self.preprocessor(self.BUILTINS.split('\n'))
            else:
                token_generator = self.preprocess_file(filename)
            
            for tokens in token_generator:
                #self.debug_parser(tokens)

                # Always allow a label as the first token.
                tokens = self.maybe_parse_label(tokens)

                # Nothing else left on this line
                if not tokens:
                    continue

                # Parse namespaces. We need those to qualify any macro names we
                # encounter.
                if self.is_word(tokens[0], ".pushns"):
                    self.parse_dot_pushns(tokens)

                elif self.is_word(tokens[0], ".popns"):
                    self.parse_dot_popns(tokens)

                elif self.is_word(tokens[0], ".scope"):
                    self.parse_dot_scope(tokens)
                    self.debug_parser("scope now {}".format(self.ns[-1]))

                elif self.is_word(tokens[0], ".endscope"):
                    self.debug_parser("endscope {}".format(self.ns[-1]))
                    self.parse_dot_endscope(tokens)

                # Opportunistically parse .equs too, while we're at it
                elif self.is_word(tokens[0], ".equ"):
                    self.parse_dot_equ(tokens)

                # Parse a macro definition
                elif tokens[0] == (self.TKT_WORD, ".macro"):
                    # .macro was parsed and verified on the previous pass. Just
                    # update our state here, and temporarily suspend macro *expansion*.
                    macro_def = True
                    self.expand_macros = False

                elif tokens[0] in [ (self.TKT_WORD, '.end'), (self.TKT_WORD, '.endmacro') ]:
                    # .endmacro was parsed and verified on the previous
                    # pass. Just update our state here.
                    macro_def = None
                    self.expand_macros = True

                elif macro_def is not None:
                    # Ignore macro definitions here.
                    continue

                elif self.is_directive(tokens[0]):
                    # But report unknown directives
                    directive_parser = self.check_valid_directive(tokens[0])
                    if directive_parser is not None:
                        directive_parser(tokens)
                    else:
                        self.warn("Unimplemented directive {}".format(tokens[0][1]))

                else:
                    self.assemble(tokens)

            # Sanity checks at end of file.
            pass


    ###############################################################################
    #
    # THE ASSEMBLER PROPER
    #
    ###############################################################################
    
    def poke(self, val, addr=None, poketype=PKT_UNKNOWN):
        """Store an value in the memory image. The optional address may be used to
        override the current address.
        """
        self.jumped = False

        # Write nothing before the final pass
        if not self.final_pass:
            return

        # if poketype != self.PKT_REG and self.opts.banked and self.bank is None:
        #     self.fail("assembling code before setting a bank (use .bank XX).")

        # Sanity check.
        # if poketype == self.PKT_INSTR:
        #     if (addr < self.opts.min_addr) and not self.boundwarn_low:
        #         self.boundwarn_low = True
        #         self.warn('Assembling code at address %06x, object file contains %06x-%06x' % \
        #                       (addr, self.opts.min_addr, self.opts.max_addr))
        #     elif (addr > self.opts.max_addr) and not self.boundwarn_high:
        #         self.boundwarn_high = True
        #         self.warn('Assembling code at address %06x, object file contains %06x-%06x' % \
        #                       (addr, self.opts.min_addr, self.opts.max_addr))

        self.minaddr = min(self.minaddr, self.addr)
        self.maxaddr = max(self.maxaddr, self.addr)

        # Okay, write it.
        if addr is None:
            addr = self.addr
        addr = addr & self.addr_mask

        if self.final_pass and self.sentinel[addr] not in (self.PKT_UNUSED, self.PKT_FILL):
            self.warn("Writing {} &{:>04x} to address {} overwrites previous {} &{:>04x}".format(
                self.POKETYPES.get(poketype, 'unknown_data'),
                val,
                self.fmt_addr(addr),
                self.POKETYPES.get(self.sentinel[addr], 'unknown data'),
                self.mem[addr]))

        # Debug it
        if self.args.debug_pokes:
            self.debug_pokes("{}: {:>04x} ; type={} ({})".format(
                self.fmt_addr(addr), val & 0xffff,
                poketype, self.POKETYPES.get(poketype, "unknown data")))

        self.logasm(addr, val, orgline=self.line)
        
        self.mem[addr] = val & 0xffff
        self.sentinel[addr] = poketype


    def poke_packed_string(self, ps):
        """Assemble a packed string at the current address."""
        # Do nothing if there's nothing to be done.
        if len(ps) == 0:
            return

        # Warning: minor acts of voodoo
        wordval, shift = 0, 0
        for char in ps:
            wordval |= (char & 0xff) << shift
            shift = shift ^ 8
            if shift == 0:
                self.poke(wordval, poketype=self.PKT_STRP)
                self.inc_addr()
                wordval = 0

        # An odd number of characters were output.
        if shift == 8:
            self.poke(wordval, poketype=self.PKT_STRP)
            self.inc_addr()


    def assemble(self, tokens):
        """Assemble a line."""

        # The parser has already isolated and parsed label definitions. All we
        # get here is assembly.
        value = self.assemble_single_field(tokens, masked=True)
        if self.final_pass:
            self.debug_assembly("Assemble: &{}: &{:>04x} ; {}".format(self.fmt_addr(), value, tokens))

        if value is None:
            # One or more values are undefined. In the final pass, a parse
            # error will be thrown and we never get here.
            self.poke(0xdead, poketype=self.PKT_UNRESOLVED)
            self.inc_addr()
        else:
            self.poke(value, poketype=self.PKT_INSTR)
            self.inc_addr()


    ###############################################################################
    #
    # SANITY CHECKING THE PARSED ASSEMBLY
    #
    ###############################################################################

    def sanity_checks(self):
        unresolved_symbols = [ x for x in self.symbols.values() if x.unresolved ]
        if unresolved_symbols:
            tmp_filename, tmp_linenum = self.filename, self.linenum
            for sym in unresolved_symbols:
                self.filename, self.linenum = sym.filename, sym.linenum
                self.error("Unresolved symbol: {} ({} undefined)".format(
                    sym.name,
                    ', '.join(x for x in sym.tokens if type(x) == str )))
            sys.exit(1)


    ###############################################################################
    #
    # OUTPUT FUNCTIONALITY
    #
    ###############################################################################

    def logasm(self, addr, val, msg=None, orgline=None):
        """Logs assembled data.

        Assembly messages log the entire program in a format that is
        human-readable but may be parsed by both the assembler and other
        machine tools.

        If val is None, then no machine-code value is dumped for this
        line. This is used to log multi-word directives etc.

        """
        # This only works for the final pass, where everything is fully
        # resolved.
        if not self.final_pass:
            return
        
        if val is not None and msg is not None:
            raise ValueError('Exactly one of val, msg may be set.')

        if msg is None:
          msg = ''

        orgline = orgline or ''
        if self.logasm_state == (self.linenum, orgline):
            orgline = ''
        else:
            self.logasm_state = (self.linenum, orgline)

        if val is None:
            line = ("&{}:      {} ;\t{}").format(self.fmt_addr(addr), msg, orgline.rstrip())
        else:
            line = ("&{}: &{:>04x} ;\t{}").format(self.fmt_addr(addr), val, orgline.rstrip())

        # if self.args.debug_assembly:
        #     print(line)
        self.pasm.write(line + '\n')


    def write_symbol_table(self):
        """Print out the symbol table."""
        # Open the symbol output file.
        try:
            with open(self.symbol_file, 'wt') as out:

                maxnamelength = max(len(x) for x in list(self.symbols.keys()))

                # TODO: make this more Python-3
                fmt = "%%-%ds  %%04x  %%-6s  %%s:%%d\n" % maxnamelength
                hdr = "%%-%ds  %%-4s  %%-6s  %%s\n" % maxnamelength
                header = hdr % ('Name', 'Val', 'Type', 'Defined in')

                out.write(header + ('-' * 79) + '\n')
                for sym in sorted(list(self.symbols.values()), key=lambda x: (x.filename, x.linenum)):
                    if sym.filename == '<builtins>':
                        continue
                    out.write(fmt % (sym.name, sym.val,
                                     sym.__class__.__name__.lower(),
                                     sym.filename, sym.linenum or '?'))

        except Exception as e:
            if self.args.debug:
                raise
            self.die("Whoops, failed to write symbol table: {}".format(e))


    def write_map_file(self):
        """Print out the map file."""
        # Open the map output file.
        with open(self.map_file, "wt") as out:
            labels = [ x for x in list(self.symbols.values())
                       if isinstance(x, Label) ]
            if not labels:
                out.write('nothing  0000  <none>:0\n')
                return

            fmt = '%%-%ds  %%04x  %%s:%%d\n' % max(len(x.name) for x in labels)

            for sym in sorted(labels, key=lambda x: (x.val, x.filename, x.linenum, x.name)):
                out.write(fmt % (sym.name, sym.val, sym.filename, sym.linenum))

            # Close it, just in case.
            out.close()


    def write_object(self):
        """Write object code."""
        #print(self.minaddr, self.maxaddr, len(self.mem[self.minaddr : self.maxaddr + 1]))
        with open(self.args.out_file, 'wb') as f:
            self.mem[self.minaddr : self.maxaddr + 1].tofile(f)
            if self.args.blocksize > 0:
                fs, bs = f.tell(), self.args.blocksize * 2048
                pad = bs * ((fs + (bs - 1)) // bs) - fs
                f.write(b"\0" * pad)

        if self.args.verilog:
            basename = os.path.splitext(self.args.out_file)[0]

            if self.model == self.MODEL_LONG and \
               self.minaddr < 0x800000 and self.maxaddr > 0x80000:
                lo_ram = open('{}-ram-00.list'.format(basename), 'wt')
                hi_ram = open('{}-ram-01.list'.format(basename), 'wt')
                maxaddr = min(0x800000, (self.args.verilog_ram_size << 10) + 1)
                lo_ram.write('\n'.join(bin(val & 0xff)[2:] for val in self.mem[self.minaddr : maxaddr]))
                hi_ram.write('\n'.join(bin(val >> 8)[2:]   for val in self.mem[self.minaddr : maxaddr]))
                # for val in self.mem[self.minaddr : 0x800000]:
                #     lo_ram.write("{:>08b}\n".format(val & 0xff))
                #     hi_ram.write("{:>08b}\n".format(val >> 8))
                lo_ram.close()
                hi_ram.close()

                lo_rom = open('{}-rom-00.list'.format(basename), 'wt')
                hi_rom = open('{}-rom-01.list'.format(basename), 'wt')
                maxaddr = 0x800000 + min(self.memsize, (self.args.verilog_rom_size << 10) + 1)
                lo_rom.write('\n'.join(bin(val & 0xff)[2:] for val in self.mem[0x800000 : maxaddr]))
                hi_rom.write('\n'.join(bin(val >> 8)[2:]   for val in self.mem[0x800000 : maxaddr]))

                # for val in self.mem[0x800000 : self.maxaddr]:
                #     lo_rom.write("{:>08b}\n".format(val & 0xff))
                #     hi_rom.write("{:>08b}\n".format(val >> 8))
                lo_rom.close()
                hi_rom.close()

            else:
                lo_ram = open('{}-00.list'.format(basename), 'wt')
                hi_ram = open('{}-01.list'.format(basename), 'wt')
                for val in self.mem[self.minaddr : self.maxaddr + 1]:
                    lo_ram.write("{:>08b}\n".format(val & 0xff))
                    hi_ram.write("{:>08b}\n".format(val >> 8))
                lo_ram.close()
                hi_ram.close()
            

if __name__ == "__main__":
    CFTAssembler().run()

# End of file.
